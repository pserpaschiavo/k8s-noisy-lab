[2025-05-11 15:20:34] Iniciando experimento: firsts-analisys-2
[2025-05-11 15:20:34] Log sendo salvo em: /home/phil/Projects/k8s-noisy-lab/results/2025-05-11/15-20-34/firsts-analisys-2/experiment.log
[2025-05-11 15:20:35] Verificando pr√©-requisitos...
[2025-05-11 15:20:35] M√©tricas ser√£o salvas em: /home/phil/Projects/k8s-noisy-lab/results/2025-05-11/15-20-34/firsts-analisys-2
[2025-05-11 15:20:35] Intervalo de coleta: 5 segundos
[2025-05-11 15:20:35] Validando recursos do cluster...
[0;35m==========================================[0m
[0;35m     VERIFICA√á√ÉO PR√â-EXPERIMENTO         [0m
[0;35m==========================================[0m
[0;35mVerificando conectividade com o cluster...[0m
[0;32m‚úì Conex√£o com o cluster estabelecida com sucesso[0m
[0;35mVerificando estado dos n√≥s do cluster...[0m
[0;32m‚úì O cluster possui 1 n√≥s dispon√≠veis[0m
[0;35mVerificando recursos dispon√≠veis no cluster...[0m
CPU: [0;35m11[0m cores
Mem√≥ria: [0;35m28.73[0m GB
[0;32m‚úì Recursos do cluster s√£o suficientes para o experimento[0m
[0;35mVerificando namespaces necess√°rias...[0m
[0;32m‚úì Namespace 'tenant-a' encontrada[0m
[0;32m‚úì Namespace 'tenant-b' encontrada[0m
[0;32m‚úì Namespace 'tenant-c' encontrada[0m
[0;32m‚úì Namespace 'monitoring' encontrada[0m
[0;35mVerificando disponibilidade do Prometheus...[0m
[0;32m‚úì Prometheus est√° rodando e pronto[0m
[0;35mVerificando disponibilidade das m√©tricas essenciais...[0m
[0;32m‚úì M√©trica 'container_cpu_usage_seconds_total' dispon√≠vel[0m
[0;32m‚úì M√©trica 'container_memory_working_set_bytes' dispon√≠vel[0m
[0;32m‚úì M√©trica 'container_network_receive_bytes_total' dispon√≠vel[0m
[0;32m‚úì M√©trica 'container_network_transmit_bytes_total' dispon√≠vel[0m
[0;32m‚úì M√©trica 'container_cpu_cfs_throttled_periods_total' dispon√≠vel[0m
[0;32m‚úì M√©trica 'container_cpu_cfs_periods_total' dispon√≠vel[0m
[0;35mVerificando permiss√µes necess√°rias...[0m
[0;35mVerificando CRDs do Prometheus Operator...[0m
[0;32m‚úì CRD 'servicemonitors.monitoring.coreos.com' encontrada[0m
[0;32m‚úì CRD 'prometheusrules.monitoring.coreos.com' encontrada[0m
[0;35m==========================================[0m
[0;32m‚úì Cluster validado com sucesso para o experimento[0m
[0;35m==========================================[0m

Informa√ß√µes adicionais:
- O experimento utiliza m√©tricas da cAdvisor integradas ao kubelet
- As queries do Prometheus usam principalmente m√©tricas rate() com 1m de janela
- Os workloads s√£o distribu√≠dos em 3 tenants para simular o cen√°rio de noisy neighbour

Recomenda√ß√µes finais:
- Certifique-se de ter pelo menos 15-20% de recursos livres al√©m dos solicitados
- Se estiver usando Minikube, use a op√ß√£o '--driver=docker' para melhor desempenho
- Verifique se n√£o h√° outros processos consumindo recursos significativos nos n√≥s
[2025-05-11 15:20:38] Criando namespaces...
[2025-05-11 15:20:38] Namespace 'tenant-a' j√° existe.
[2025-05-11 15:20:38] Namespace 'tenant-b' j√° existe.
[2025-05-11 15:20:39] Namespace 'tenant-c' j√° existe.
[2025-05-11 15:20:39] Namespace 'tenant-d' j√° existe.
[2025-05-11 15:20:39] Namespace 'monitoring' j√° existe.
[2025-05-11 15:20:39] Namespace 'ingress-nginx' j√° existe.
NAME              STATUS   AGE
default           Active   58m
ingress-nginx     Active   58m
kube-node-lease   Active   58m
kube-public       Active   58m
kube-system       Active   58m
metallb-system    Active   53m
monitoring        Active   58m
tenant-a          Active   58m
tenant-b          Active   58m
tenant-c          Active   58m
tenant-d          Active   58m
[2025-05-11 15:20:44] Aplicando resource quotas...
resourcequota/tenant-a-quota configured
resourcequota/tenant-b-quota unchanged
resourcequota/tenant-c-quota configured
resourcequota/tenant-d-quota unchanged
[2025-05-11 15:20:49] Verificando quotas de recursos...
NAMESPACE   NAME             AGE   REQUEST                                                 LIMIT
tenant-a    tenant-a-quota   51m   requests.cpu: 800m/1500m, requests.memory: 1Gi/2Gi      limits.cpu: 1600m/3, limits.memory: 2Gi/4Gi
tenant-b    tenant-b-quota   51m   requests.cpu: 2500m/3, requests.memory: 2816Mi/4Gi      limits.cpu: 5800m/6, limits.memory: 7680Mi/8Gi
tenant-c    tenant-c-quota   51m   requests.cpu: 350m/1500m, requests.memory: 1088Mi/3Gi   limits.cpu: 600m/3, limits.memory: 1664Mi/6Gi
tenant-d    tenant-d-quota   51m   requests.cpu: 600m/2, requests.memory: 1152Mi/2Gi       limits.cpu: 1200m/4, limits.memory: 1792Mi/4Gi
[2025-05-11 15:20:49] Namespace 'monitoring' j√° existe.
[2025-05-11 15:20:49] Verificando o estado do Prometheus...
[2025-05-11 15:20:49] Verificando pods do Prometheus...
[2025-05-11 15:20:49] Verificando servi√ßos do Prometheus...
[2025-05-11 15:20:50] Encontrado servi√ßo do Prometheus: prometheus-operated
[2025-05-11 15:20:52] Configurando port-forward para o Prometheus...
Unable to listen on port 9090: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:9090: bind: address already in use unable to create listener: Error listen tcp6 [::1]:9090: bind: address already in use]
error: unable to listen on any of the requested ports: [{9090 9090}]
[2025-05-11 15:20:57] Prometheus est√° acess√≠vel via localhost:9090
[2025-05-11 15:20:57] Prometheus tem 73 targets ativos
[2025-05-11 15:20:57] Verifica√ß√£o do Prometheus conclu√≠da
[2025-05-11 15:20:57] Ajustando dura√ß√µes nos manifestos de acordo com as dura√ß√µes das fases...
[2025-05-11 15:20:57] Dura√ß√£o total do experimento (estimada): 1800s para 1 rounds
[2025-05-11 15:20:57] Dura√ß√£o m√≠nima dos workloads: 2340s (inclui margem de seguran√ßa de 30%)
[2025-05-11 15:20:57]   - Ajustada dura√ß√£o do nginx-benchmark para 2340s
[2025-05-11 15:20:57]   - Ajustada dura√ß√£o do continuous-memory-stress para 2340s
[2025-05-11 15:20:57]   - Ajustada dura√ß√£o dos workloads cont√≠nuos do tenant-d para 2340s
[2025-05-11 15:20:57]   - Ajustado timeout do stress-ng para 960s
[2025-05-11 15:20:57] Manifestos ajustados com sucesso!
[2025-05-11 15:20:57]   - Verificadas configura√ß√µes adicionais de dura√ß√£o no tenant-a
[2025-05-11 15:20:57] ======= IN√çCIO DO EXPERIMENTO: firsts-analisys-2 =======
[2025-05-11 15:20:57] Data: 2025/05/11, Hora: 15:20:34
[2025-05-11 15:20:57] N√∫mero de rounds: 1
[2025-05-11 15:20:57] Dura√ß√£o das fases: Baseline=300s, Ataque=900s, Recupera√ß√£o=600s
[2025-05-11 15:20:57] Intervalo de coleta de m√©tricas: 5s
[2025-05-11 15:20:57] ===== ROUND 1/1 =====
[2025-05-11 15:20:57] Limpando workloads para come√ßar o round 1 com um ambiente limpo...
deployment.apps "nginx-deployment" deleted
configmap "nginx-config" deleted
service "nginx" deleted
job.batch "nginx-benchmark" deleted
deployment.apps "iperf-server" deleted
service "iperf-server" deleted
deployment.apps "stress-ng" deleted
deployment.apps "traffic-generator" deleted
deployment.apps "traffic-server" deleted
configmap "nginx-traffic-server-config" deleted
service "traffic-server" deleted
service "redis" deleted
deployment.apps "redis-deployment" deleted
job.batch "continuous-memory-stress" deleted
deployment.apps "memory-monitor" deleted
persistentvolumeclaim "postgres-data" deleted
configmap "postgres-config" deleted
service "postgres" deleted
deployment.apps "postgres" deleted
job.batch "pgbench-init" deleted
job.batch "pgbench-continuous-workload" deleted
job.batch "cpu-intensive-continuous" deleted
job.batch "disk-intensive-continuous" deleted
error: error parsing /home/phil/Projects/k8s-noisy-lab/manifests/tenant-d/cpu-disk-workload.yaml: error converting YAML to JSON: yaml: line 94: could not find expected ':'
[2025-05-11 15:20:58] Aguardando finaliza√ß√£o completa dos recursos anteriores...
pod/iperf-server-d79fbdf6c-jsdtl condition met
pod/nginx-deployment-56fc57d4-75bcl condition met
pod/nginx-deployment-56fc57d4-gn54r condition met
pod/nginx-deployment-56fc57d4-tdcfm condition met
pod/stress-ng-6764fc7965-hwzsz condition met
pod/stress-ng-6764fc7965-nfg44 condition met
pod/traffic-generator-85fb77744d-dxvqk condition met
pod/traffic-generator-85fb77744d-m5v4c condition met
pod/traffic-generator-85fb77744d-nsrnx condition met
[2025-05-11 15:21:29] === Fase 1 - Baseline ===
[2025-05-11 15:21:29] Coletando m√©tricas a cada 5 segundos...
[2025-05-11 15:21:29] Coleta de m√©tricas iniciada com PID: 440448 (log: /home/phil/Projects/k8s-noisy-lab/results/2025-05-11/15-20-34/firsts-analisys-2/round-1/1 - Baseline/logs/metrics_collection.log)
[2025-05-11 15:21:29] Iniciando fase: 1 - Baseline com dura√ß√£o m√°xima de 300s
[2025-05-11 15:21:29] Implantando tenant-a (sens√≠vel √† rede)...
[2025-05-11 15:21:29] Prometheus j√° est√° acess√≠vel via localhost:9090
deployment.apps/nginx-deployment created
configmap/nginx-config created
service/nginx created
job.batch/nginx-benchmark created
deployment.apps/iperf-server created
service/iperf-server created
[2025-05-11 15:21:29] Aguardando inicializa√ß√£o dos servi√ßos do tenant-a...
deployment.apps/nginx-deployment condition met
[2025-05-11 15:21:31] Implantando tenant-c (v√≠tima)...
service/redis created
deployment.apps/redis-deployment created
job.batch/continuous-memory-stress created
deployment.apps/memory-monitor created
[2025-05-11 15:21:31] Implantando tenant-d (CPU e Disco)...
persistentvolumeclaim/postgres-data created
configmap/postgres-config created
service/postgres created
deployment.apps/postgres created
job.batch/pgbench-init created
job.batch/pgbench-continuous-workload created
job.batch/cpu-intensive-continuous created
job.batch/disk-intensive-continuous created
error: error parsing /home/phil/Projects/k8s-noisy-lab/manifests/tenant-d/cpu-disk-workload.yaml: error converting YAML to JSON: yaml: line 94: could not find expected ':'
[2025-05-11 15:26:29] Fase 1 - Baseline conclu√≠da
[2025-05-11 15:26:29] Interrompendo coleta de m√©tricas (PID: 440448)...
[2025-05-11 15:26:29] Coleta de m√©tricas finalizada.
[2025-05-11 15:26:29] Fase '1 - Baseline' conclu√≠da.
[2025-05-11 15:26:29] Modo n√£o interativo: continuando automaticamente para a pr√≥xima fase...
[2025-05-11 15:26:29] === Fase 2 - Attack ===
[2025-05-11 15:26:29] Coletando m√©tricas a cada 5 segundos...
[2025-05-11 15:26:29] Coleta de m√©tricas iniciada com PID: 497322 (log: /home/phil/Projects/k8s-noisy-lab/results/2025-05-11/15-20-34/firsts-analisys-2/round-1/2 - Attack/logs/metrics_collection.log)
[2025-05-11 15:26:29] Iniciando fase: 2 - Attack com dura√ß√£o m√°xima de 900s
[2025-05-11 15:26:29] Implantando tenant-b (atacante noisy neighbor)...
[2025-05-11 15:26:29] Prometheus j√° est√° acess√≠vel via localhost:9090
deployment.apps/stress-ng created
deployment.apps/traffic-generator created
deployment.apps/traffic-server created
configmap/nginx-traffic-server-config created
service/traffic-server created
[2025-05-11 15:26:30] Aguardando inicializa√ß√£o dos servi√ßos do tenant-b...
deployment.apps/traffic-generator condition met
[2025-05-11 15:28:21] Timeout aguardando pelo traffic-server no tenant-b
deployment.apps/stress-ng condition met
Error from server (NotFound): deployments.apps "iperf-server" not found
[2025-05-11 15:28:21] Timeout aguardando pelo iperf-server no tenant-b
[2025-05-11 15:28:21] Verificando todos os tenants ap√≥s a implanta√ß√£o do atacante...
[2025-05-11 15:28:21] Verificando se todos os tenants relevantes para a fase 'attack' est√£o prontos...
[2025-05-11 15:28:21] Aguardando pods no namespace tenant-a ficarem prontos (timeout: 60s)...
[2025-05-11 15:28:21] Todos os pods no namespace tenant-a est√£o prontos!
[2025-05-11 15:28:22] Aguardando pods no namespace tenant-b ficarem prontos (timeout: 60s)...
[2025-05-11 15:28:22] Todos os pods no namespace tenant-b est√£o prontos!
[2025-05-11 15:28:22] Aguardando pods no namespace tenant-c ficarem prontos (timeout: 60s)...
[2025-05-11 15:28:22] Todos os pods no namespace tenant-c est√£o prontos!
[2025-05-11 15:28:22] Aguardando pods no namespace tenant-d ficarem prontos (timeout: 60s)...
[2025-05-11 15:28:23] Status dos pods em tenant-d:
[2025-05-11 15:28:23] Detalhes dos pods com problemas:
[2025-05-11 15:28:28] Aguardando pods no namespace tenant-d... (5/60s) - 4 pods ainda n√£o prontos, 0 n√£o em execu√ß√£o
[2025-05-11 15:28:34] Aguardando pods no namespace tenant-d... (10/60s) - 4 pods ainda n√£o prontos, 0 n√£o em execu√ß√£o
[2025-05-11 15:28:39] Status dos pods em tenant-d:
[2025-05-11 15:28:39] Detalhes dos pods com problemas:
[2025-05-11 15:28:44] Aguardando pods no namespace tenant-d... (20/60s) - 4 pods ainda n√£o prontos, 0 n√£o em execu√ß√£o
[2025-05-11 15:28:50] Aguardando pods no namespace tenant-d... (25/60s) - 4 pods ainda n√£o prontos, 0 n√£o em execu√ß√£o
[2025-05-11 15:28:55] Status dos pods em tenant-d:
[2025-05-11 15:28:55] Detalhes dos pods com problemas:
[2025-05-11 15:29:01] Aguardando pods no namespace tenant-d... (35/60s) - 4 pods ainda n√£o prontos, 0 n√£o em execu√ß√£o
[2025-05-11 15:29:06] Aguardando pods no namespace tenant-d... (40/60s) - 4 pods ainda n√£o prontos, 0 n√£o em execu√ß√£o
[2025-05-11 15:29:11] Status dos pods em tenant-d:
[2025-05-11 15:29:11] Detalhes dos pods com problemas:
[2025-05-11 15:29:17] Aguardando pods no namespace tenant-d... (50/60s) - 4 pods ainda n√£o prontos, 0 n√£o em execu√ß√£o
[2025-05-11 15:29:22] Aguardando pods no namespace tenant-d... (55/60s) - 4 pods ainda n√£o prontos, 0 n√£o em execu√ß√£o
[2025-05-11 15:29:27] Timeout aguardando pods no namespace tenant-d
[2025-05-11 15:29:27] Status atual dos pods:
[2025-05-11 15:29:27] Problemas detectados no namespace tenant-d
[2025-05-11 15:29:27] Aguardando pods no namespace ingress-nginx ficarem prontos (timeout: 60s)...
[2025-05-11 15:29:28] Todos os pods no namespace ingress-nginx est√£o prontos!
[2025-05-11 15:29:28] Nem todos os tenants est√£o completamente prontos, verificando se √© poss√≠vel continuar...
[2025-05-11 15:29:28] Os pods cr√≠ticos est√£o funcionando. Continuando, mas os resultados podem ser afetados.
