[2025-05-11 14:55:53] Iniciando experimento: firsts-analisys-1
[2025-05-11 14:55:53] Log sendo salvo em: /home/phil/Projects/k8s-noisy-lab/results/2025-05-11/14-55-53/firsts-analisys-1/experiment.log
[2025-05-11 14:55:53] Verificando pr√©-requisitos...
[2025-05-11 14:55:53] M√©tricas ser√£o salvas em: /home/phil/Projects/k8s-noisy-lab/results/2025-05-11/14-55-53/firsts-analisys-1
[2025-05-11 14:55:53] Intervalo de coleta: 5 segundos
[2025-05-11 14:55:53] Validando recursos do cluster...
[0;35m==========================================[0m
[0;35m     VERIFICA√á√ÉO PR√â-EXPERIMENTO         [0m
[0;35m==========================================[0m
[0;35mVerificando conectividade com o cluster...[0m
[0;32m‚úì Conex√£o com o cluster estabelecida com sucesso[0m
[0;35mVerificando estado dos n√≥s do cluster...[0m
[0;32m‚úì O cluster possui 1 n√≥s dispon√≠veis[0m
[0;35mVerificando recursos dispon√≠veis no cluster...[0m
CPU: [0;35m11[0m cores
Mem√≥ria: [0;35m28.73[0m GB
[0;32m‚úì Recursos do cluster s√£o suficientes para o experimento[0m
[0;35mVerificando namespaces necess√°rias...[0m
[0;32m‚úì Namespace 'tenant-a' encontrada[0m
[0;32m‚úì Namespace 'tenant-b' encontrada[0m
[0;32m‚úì Namespace 'tenant-c' encontrada[0m
[0;32m‚úì Namespace 'monitoring' encontrada[0m
[0;35mVerificando disponibilidade do Prometheus...[0m
[0;32m‚úì Prometheus est√° rodando e pronto[0m
[0;35mVerificando disponibilidade das m√©tricas essenciais...[0m
[0;32m‚úì M√©trica 'container_cpu_usage_seconds_total' dispon√≠vel[0m
[0;32m‚úì M√©trica 'container_memory_working_set_bytes' dispon√≠vel[0m
[0;32m‚úì M√©trica 'container_network_receive_bytes_total' dispon√≠vel[0m
[0;32m‚úì M√©trica 'container_network_transmit_bytes_total' dispon√≠vel[0m
[0;32m‚úì M√©trica 'container_cpu_cfs_throttled_periods_total' dispon√≠vel[0m
[0;32m‚úì M√©trica 'container_cpu_cfs_periods_total' dispon√≠vel[0m
[0;35mVerificando permiss√µes necess√°rias...[0m
[0;35mVerificando CRDs do Prometheus Operator...[0m
[0;32m‚úì CRD 'servicemonitors.monitoring.coreos.com' encontrada[0m
[0;32m‚úì CRD 'prometheusrules.monitoring.coreos.com' encontrada[0m
[0;35m==========================================[0m
[0;32m‚úì Cluster validado com sucesso para o experimento[0m
[0;35m==========================================[0m

Informa√ß√µes adicionais:
- O experimento utiliza m√©tricas da cAdvisor integradas ao kubelet
- As queries do Prometheus usam principalmente m√©tricas rate() com 1m de janela
- Os workloads s√£o distribu√≠dos em 3 tenants para simular o cen√°rio de noisy neighbour

Recomenda√ß√µes finais:
- Certifique-se de ter pelo menos 15-20% de recursos livres al√©m dos solicitados
- Se estiver usando Minikube, use a op√ß√£o '--driver=docker' para melhor desempenho
- Verifique se n√£o h√° outros processos consumindo recursos significativos nos n√≥s
[2025-05-11 14:55:56] Criando namespaces...
[2025-05-11 14:55:56] Namespace 'tenant-a' j√° existe.
[2025-05-11 14:55:56] Namespace 'tenant-b' j√° existe.
[2025-05-11 14:55:57] Namespace 'tenant-c' j√° existe.
[2025-05-11 14:55:57] Namespace 'tenant-d' j√° existe.
[2025-05-11 14:55:57] Namespace 'monitoring' j√° existe.
[2025-05-11 14:55:57] Namespace 'ingress-nginx' j√° existe.
NAME              STATUS   AGE
default           Active   33m
ingress-nginx     Active   33m
kube-node-lease   Active   33m
kube-public       Active   33m
kube-system       Active   33m
metallb-system    Active   28m
monitoring        Active   33m
tenant-a          Active   33m
tenant-b          Active   33m
tenant-c          Active   33m
tenant-d          Active   33m
[2025-05-11 14:56:02] Aplicando resource quotas...
resourcequota/tenant-a-quota configured
resourcequota/tenant-b-quota unchanged
resourcequota/tenant-c-quota configured
resourcequota/tenant-d-quota unchanged
[2025-05-11 14:56:07] Verificando quotas de recursos...
NAMESPACE   NAME             AGE   REQUEST                                                 LIMIT
tenant-a    tenant-a-quota   27m   requests.cpu: 800m/1500m, requests.memory: 1Gi/2Gi      limits.cpu: 1600m/3, limits.memory: 2Gi/4Gi
tenant-b    tenant-b-quota   27m   requests.cpu: 0/3, requests.memory: 0/4Gi               limits.cpu: 0/6, limits.memory: 0/8Gi
tenant-c    tenant-c-quota   27m   requests.cpu: 550m/1500m, requests.memory: 1344Mi/3Gi   limits.cpu: 1/3, limits.memory: 2176Mi/6Gi
tenant-d    tenant-d-quota   27m   requests.cpu: 600m/2, requests.memory: 1152Mi/2Gi       limits.cpu: 1200m/4, limits.memory: 1792Mi/4Gi
[2025-05-11 14:56:07] Namespace 'monitoring' j√° existe.
[2025-05-11 14:56:07] Verificando o estado do Prometheus...
[2025-05-11 14:56:07] Verificando pods do Prometheus...
[2025-05-11 14:56:07] Verificando servi√ßos do Prometheus...
[2025-05-11 14:56:07] Encontrado servi√ßo do Prometheus: prometheus-operated
[2025-05-11 14:56:09] Configurando port-forward para o Prometheus...
Unable to listen on port 9090: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:9090: bind: address already in use unable to create listener: Error listen tcp6 [::1]:9090: bind: address already in use]
error: unable to listen on any of the requested ports: [{9090 9090}]
[2025-05-11 14:56:14] Prometheus est√° acess√≠vel via localhost:9090
[2025-05-11 14:56:14] Prometheus tem 68 targets ativos
[2025-05-11 14:56:14] Verifica√ß√£o do Prometheus conclu√≠da
[2025-05-11 14:56:15] Ajustando dura√ß√µes nos manifestos de acordo com as dura√ß√µes das fases...
[2025-05-11 14:56:15] Dura√ß√£o total do experimento (estimada): 1800s para 1 rounds
[2025-05-11 14:56:15] Dura√ß√£o m√≠nima dos workloads: 2340s (inclui margem de seguran√ßa de 30%)
[2025-05-11 14:56:15]   - Ajustada dura√ß√£o do nginx-benchmark para 2340s
[2025-05-11 14:56:15]   - Ajustada dura√ß√£o do continuous-memory-stress para 2340s
[2025-05-11 14:56:15]   - Ajustada dura√ß√£o dos workloads cont√≠nuos do tenant-d para 2340s
[2025-05-11 14:56:15]   - Ajustado timeout do stress-ng para 960s
[2025-05-11 14:56:15] Manifestos ajustados com sucesso!
[2025-05-11 14:56:15]   - Verificadas configura√ß√µes adicionais de dura√ß√£o no tenant-a
[2025-05-11 14:56:15] ======= IN√çCIO DO EXPERIMENTO: firsts-analisys-1 =======
[2025-05-11 14:56:15] Data: 2025/05/11, Hora: 14:55:53
[2025-05-11 14:56:15] N√∫mero de rounds: 1
[2025-05-11 14:56:15] Dura√ß√£o das fases: Baseline=300s, Ataque=900s, Recupera√ß√£o=600s
[2025-05-11 14:56:15] Intervalo de coleta de m√©tricas: 5s
[2025-05-11 14:56:15] ===== ROUND 1/1 =====
[2025-05-11 14:56:15] Limpando workloads para come√ßar o round 1 com um ambiente limpo...
deployment.apps "nginx-deployment" deleted
configmap "nginx-config" deleted
service "nginx" deleted
job.batch "nginx-benchmark" deleted
deployment.apps "iperf-server" deleted
service "iperf-server" deleted
service "redis" deleted
deployment.apps "redis-deployment" deleted
job.batch "continuous-memory-stress" deleted
deployment.apps "memory-monitor" deleted
persistentvolumeclaim "postgres-data" deleted
configmap "postgres-config" deleted
service "postgres" deleted
deployment.apps "postgres" deleted
job.batch "pgbench-init" deleted
job.batch "pgbench-continuous-workload" deleted
job.batch "cpu-intensive-continuous" deleted
job.batch "disk-intensive-continuous" deleted
error: error parsing /home/phil/Projects/k8s-noisy-lab/manifests/tenant-d/cpu-disk-workload.yaml: error converting YAML to JSON: yaml: line 94: could not find expected ':'
[2025-05-11 14:56:15] Aguardando finaliza√ß√£o completa dos recursos anteriores...
pod/iperf-server-d79fbdf6c-qjqz9 condition met
pod/nginx-deployment-56fc57d4-fpfz9 condition met
pod/nginx-deployment-56fc57d4-klrxk condition met
pod/nginx-deployment-56fc57d4-kqmzs condition met
pod/continuous-memory-stress-mrb4p condition met
pod/memory-monitor-cfd7584c6-r6ptx condition met
[2025-05-11 14:56:46] === Fase 1 - Baseline ===
[2025-05-11 14:56:46] Coleta de m√©tricas iniciada com PID: 315713 (log: /home/phil/Projects/k8s-noisy-lab/results/2025-05-11/14-55-53/firsts-analisys-1/round-1/1 - Baseline/logs/metrics_collection.log)
[2025-05-11 14:56:46] Coletando m√©tricas a cada 5 segundos...
[2025-05-11 14:56:46] Iniciando fase: 1 - Baseline com dura√ß√£o m√°xima de 300s
[2025-05-11 14:56:46] Implantando tenant-a (sens√≠vel √† rede)...
[2025-05-11 14:56:46] Prometheus j√° est√° acess√≠vel via localhost:9090
deployment.apps/nginx-deployment created
configmap/nginx-config created
service/nginx created
job.batch/nginx-benchmark created
deployment.apps/iperf-server created
service/iperf-server created
[2025-05-11 14:56:46] Aguardando inicializa√ß√£o dos servi√ßos do tenant-a...
deployment.apps/nginx-deployment condition met
[2025-05-11 14:56:48] Implantando tenant-c (v√≠tima)...
service/redis created
deployment.apps/redis-deployment created
job.batch/continuous-memory-stress created
deployment.apps/memory-monitor created
[2025-05-11 14:56:48] Implantando tenant-d (CPU e Disco)...
persistentvolumeclaim/postgres-data created
configmap/postgres-config created
service/postgres created
deployment.apps/postgres created
job.batch/pgbench-init created
job.batch/pgbench-continuous-workload created
job.batch/cpu-intensive-continuous created
job.batch/disk-intensive-continuous created
error: error parsing /home/phil/Projects/k8s-noisy-lab/manifests/tenant-d/cpu-disk-workload.yaml: error converting YAML to JSON: yaml: line 94: could not find expected ':'
[2025-05-11 15:01:46] Fase 1 - Baseline conclu√≠da
[2025-05-11 15:01:46] Interrompendo coleta de m√©tricas (PID: 315713)...
[2025-05-11 15:01:46] Coleta de m√©tricas finalizada.
[2025-05-11 15:01:46] Fase '1 - Baseline' conclu√≠da.
[2025-05-11 15:01:46] Modo n√£o interativo: continuando automaticamente para a pr√≥xima fase...
[2025-05-11 15:01:46] === Fase 2 - Attack ===
[2025-05-11 15:01:46] Coleta de m√©tricas iniciada com PID: 375840 (log: /home/phil/Projects/k8s-noisy-lab/results/2025-05-11/14-55-53/firsts-analisys-1/round-1/2 - Attack/logs/metrics_collection.log)
[2025-05-11 15:01:46] Coletando m√©tricas a cada 5 segundos...
[2025-05-11 15:01:46] Iniciando fase: 2 - Attack com dura√ß√£o m√°xima de 900s
[2025-05-11 15:01:46] Implantando tenant-b (atacante noisy neighbor)...
[2025-05-11 15:01:46] Prometheus j√° est√° acess√≠vel via localhost:9090
deployment.apps/stress-ng created
deployment.apps/traffic-generator created
deployment.apps/traffic-server created
configmap/nginx-traffic-server-config created
service/traffic-server created
[2025-05-11 15:01:46] Aguardando inicializa√ß√£o dos servi√ßos do tenant-b...
deployment.apps/traffic-generator condition met
error: timed out waiting for the condition on deployments/traffic-server
[2025-05-11 15:03:56] Timeout aguardando pelo traffic-server no tenant-b
deployment.apps/stress-ng condition met
Error from server (NotFound): deployments.apps "iperf-server" not found
[2025-05-11 15:03:56] Timeout aguardando pelo iperf-server no tenant-b
[2025-05-11 15:03:56] Verificando todos os tenants ap√≥s a implanta√ß√£o do atacante...
[2025-05-11 15:03:56] Verificando se todos os tenants relevantes para a fase 'attack' est√£o prontos...
[2025-05-11 15:03:56] Aguardando pods no namespace tenant-a ficarem prontos (timeout: 60s)...
[2025-05-11 15:03:56] Status dos pods em tenant-a:
[2025-05-11 15:03:56] Detalhes dos pods com problemas:
[2025-05-11 15:04:01] Aguardando pods no namespace tenant-a... (5/60s) - 1 pods ainda n√£o prontos, 0 n√£o em execu√ß√£o
[2025-05-11 15:04:07] Aguardando pods no namespace tenant-a... (10/60s) - 1 pods ainda n√£o prontos, 0 n√£o em execu√ß√£o
[2025-05-11 15:04:12] Status dos pods em tenant-a:
[2025-05-11 15:04:12] Detalhes dos pods com problemas:
[2025-05-11 15:04:17] Aguardando pods no namespace tenant-a... (20/60s) - 1 pods ainda n√£o prontos, 0 n√£o em execu√ß√£o
[2025-05-11 15:04:22] Aguardando pods no namespace tenant-a... (25/60s) - 1 pods ainda n√£o prontos, 0 n√£o em execu√ß√£o
[2025-05-11 15:04:28] Status dos pods em tenant-a:
[2025-05-11 15:04:28] Detalhes dos pods com problemas:
[2025-05-11 15:04:33] Aguardando pods no namespace tenant-a... (35/60s) - 1 pods ainda n√£o prontos, 0 n√£o em execu√ß√£o
[2025-05-11 15:04:38] Aguardando pods no namespace tenant-a... (40/60s) - 1 pods ainda n√£o prontos, 0 n√£o em execu√ß√£o
[2025-05-11 15:04:44] Status dos pods em tenant-a:
[2025-05-11 15:04:44] Detalhes dos pods com problemas:
[2025-05-11 15:04:49] Aguardando pods no namespace tenant-a... (50/60s) - 1 pods ainda n√£o prontos, 0 n√£o em execu√ß√£o
